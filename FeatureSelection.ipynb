{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianasv/HNC_PREDICTION/blob/main/FeatureSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code performs feature selection using the Spearman Correlation matrix and Random Survival Forest as feature selector in the whole dataset**"
      ],
      "metadata": {
        "id": "m09yx-8nNMEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries downloading"
      ],
      "metadata": {
        "id": "y0CQFnt6Nb8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MvKhuKqswjH0",
        "outputId": "8b1e3e42-a264-465f-ec61-01c1062d2ce6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lifelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6mx6r-UIFWVb",
        "outputId": "76cef65a-5b51-4e97-d4db-4746bc7cea62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lifelines\n",
            "  Downloading lifelines-0.29.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.7.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines)\n",
            "  Downloading formulaic-1.0.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n",
            "Downloading lifelines-0.29.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.0.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=8c7e7e20f4c6f62658613b665f838974dc364fac4c7290ae6ff7477f14ad65b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cc/e0/ef2969164144c899fedb22b338f6703e2b9cf46eeebf254991\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-1.0.2 interface-meta-1.3.0 lifelines-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-survival"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr-wA0dwBXno",
        "outputId": "0c4a2ca0-1db2-481c-980d-c2ee0a4e1a6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ecos in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.4.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.26.4)\n",
            "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (0.6.7.post0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival) (0.1.7.post4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->scikit-survival) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->scikit-survival) (1.16.0)\n",
            "Downloading scikit_survival-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-survival\n",
            "Successfully installed scikit-survival-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries importing"
      ],
      "metadata": {
        "id": "7MocZ2M5NePn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lifelines\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from sklearn.inspection import permutation_importance"
      ],
      "metadata": {
        "id": "r6NRjoR_pPAu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr_nPubaupt-",
        "outputId": "77acdbc6-a9bc-4b0a-e0b7-81267847f17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the path of the folder in drive to save the generated datasets and outputs\n",
        "folder_datasets = '/content/drive/MyDrive/Tesis/DefinitiveDatasets'\n",
        "folder_outputs = '/content/drive/MyDrive/Tesis/DefinitiveOutputs'"
      ],
      "metadata": {
        "id": "8vSt8Nf3KJCX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This URL leads to a Github repository containing the anonymized dataset with the random numbers already removed\n",
        "filename = \"https://github.com/elianasv/HNC_PREDICTION/raw/main/df_scaled.xlsx\"\n",
        "df_scaled = pd.read_excel(filename,engine='openpyxl')"
      ],
      "metadata": {
        "id": "XkWrncMSHpDE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIafWXnjuxtp"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_excel('/content/drive/MyDrive/Tesis/df_patiens_rem_reordered_renamed.xlsx') # Update file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8jd8krRZwJbT"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe containing only the continuous variables\n",
        "df_cont = df_scaled.drop(['DFS', 'DFSCensor','ID','OS','Dcd','LRC','LRCCensor','Centre','sex', 'chimio', 'stadeT', 'stadeN', 'stade', 'Tabac', 'P16','LocalisationORL'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation"
      ],
      "metadata": {
        "id": "urFd3FsErlE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = df_cont.corr(method='spearman')\n",
        "\n",
        "# Extract the upper triangle of the correlation matrix\n",
        "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Save the half matrix to an Excel file\n",
        "upper.to_excel(f'{folder_outputs}/spearman_correlation_matrix_half.xlsx')\n"
      ],
      "metadata": {
        "id": "lJgdAEHtQU4k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a varying correlation threshold\n",
        "threshold = 0.9\n",
        "\n",
        "# Initialize a list to store all correlated features with correlation values for each feature\n",
        "dropped_features_info = []\n",
        "\n",
        "# Find highly correlated features and store all correlations for each feature to drop\n",
        "for column in upper.columns:\n",
        "    high_corr = upper[column][upper[column] > threshold]\n",
        "    if not high_corr.empty:\n",
        "        correlated_features = \"; \".join([f\"{idx}: {corr_value:.6f}\" for idx, corr_value in high_corr.items()])\n",
        "        dropped_features_info.append({\n",
        "            'Feature to Drop': column,\n",
        "            'Correlations': correlated_features\n",
        "        })\n",
        "\n",
        "# Convert the list of dropped features and their correlations into a DataFrame\n",
        "df_dropped_features = pd.DataFrame(dropped_features_info)\n",
        "\n",
        "# Drop highly correlated features from the original dataframe\n",
        "df_corr = df_scaled.drop(columns=[item['Feature to Drop'] for item in dropped_features_info])\n",
        "\n",
        "# Debugging: print the number of features remaining\n",
        "print(f\"Threshold: {threshold}, Features retained: {df_corr.shape[1]}\")\n",
        "\n",
        "# Display the DataFrame with dropped features and all their correlations\n",
        "#print(df_dropped_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5iTsv14pCOx",
        "outputId": "6af05cbb-9cf3-41de-8372-bbf59a3f5405"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.9, Features retained: 139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the dataset"
      ],
      "metadata": {
        "id": "cTktyV2nN2Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped_features.to_excel(f'{folder_outputs}/df_corr_removed_features.xlsx',index=False)"
      ],
      "metadata": {
        "id": "k_CN9JT6pq0I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr.to_excel(f'{folder_datasets}/df_corr.xlsx',index=False)"
      ],
      "metadata": {
        "id": "HTiJOVbjA98Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr.to_csv(f'{folder_datasets}/df_corr.csv',index=False)"
      ],
      "metadata": {
        "id": "pggO5iyrY-bs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Survival Forest (RSF)"
      ],
      "metadata": {
        "id": "2TcWBvg6AL4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As the concordance method doesn't have its own c-index score calculator by default, this function is going to be passed\n",
        "def concordance_index_scorer(estimator, X, y):\n",
        "    \"\"\"Custom scorer function for concordance index.\"\"\"\n",
        "    # Convert the structured array 'y' to separate arrays for time and event\n",
        "    time = y['time']\n",
        "    event = y['event']\n",
        "\n",
        "    # Predict the risk scores\n",
        "    risk_scores = estimator.predict(X)\n",
        "\n",
        "    # Calculate the concordance index\n",
        "    cindex = lifelines.utils.concordance_index(time, risk_scores, event_observed=event)\n",
        "\n",
        "    return cindex\n"
      ],
      "metadata": {
        "id": "EI4PA3rzFfPj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_cont will be used to generate the survival data based on 'DFS' and 'DFSCensor'\n",
        "\n",
        "# Prepare the survival data for RSF\n",
        "# 'DFS' is the time in months, and 'DFSCensor' is the censoring indicator (1 = event, 0 = censored)\n",
        "y = np.array([(bool(event), time) for event, time in zip(df_scaled['DFSCensor'], df_scaled['DFS'])],\n",
        "             dtype=[('event', bool), ('time', float)])\n",
        "\n",
        "# Initialize the Random Survival Forest model\n",
        "rsf = RandomSurvivalForest(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit RSF on the full dataset (without splitting the data)\n",
        "rsf.fit(df_cont, y)\n",
        "\n",
        "perm_importance = permutation_importance(rsf, df_cont, y, n_repeats=10, random_state=42, scoring=concordance_index_scorer)\n",
        "\n",
        "# Create a DataFrame to rank the features by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': df_cont.columns,\n",
        "    'Importance': perm_importance.importances_mean\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cZg8nbSDFO3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the top features based on importance\n",
        "selected_features_rsf = importance_df.head(100)['Feature']  # Select the top 100 features\n",
        "print(f'Selected features: {importance_df.head(100).shape[0]}')\n",
        "selected_features_rsf.head(100).to_excel(f'{folder_outputs}/selected_features_rsf.xlsx',index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuX2p0UpJYeY",
        "outputId": "b0b8d2e1-9732-4676-960c-c2d4d639c235"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make a dataset with the noncontinuous features + selected features\n",
        "\n",
        "# Create a list of non-continuous features\n",
        "non_continuous_features = ['DFS', 'DFSCensor','ID','OS','Dcd','LRC','LRCCensor','Centre','sex', 'chimio', 'stadeT', 'stadeN', 'stade', 'Tabac', 'P16','LocalisationORL']\n",
        "\n",
        "# Create a new dataset with the non-continuous features and the selected features from RSF\n",
        "df_rsf_sel = df_scaled[non_continuous_features + list(selected_features_rsf)]\n"
      ],
      "metadata": {
        "id": "NNG-U3VbLd_g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the dataset"
      ],
      "metadata": {
        "id": "SV9RmVnQN5BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final dataset to an Excel file\n",
        "df_rsf_sel.to_excel(f'{folder_datasets}/df_rsf_sel.xlsx', index=False)"
      ],
      "metadata": {
        "id": "xmVGAM4KMJ-N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rsf_sel.to_csv(f'{folder_datasets}/df_rsf_sel.csv',index=False)"
      ],
      "metadata": {
        "id": "WGQH-_soMM9G"
      },
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TFXGqWr0d0sXkORfrRQOOP1YOsFMT5-R",
      "authorship_tag": "ABX9TyP2300VbglLPaoEoQUXh0wr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}